{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n이후에는 대회측 배포 데이터셋을 이 deep-text-recognition 디렉토리 내에 받아 open 이라는 디렉토리를 만들어 줍니다.\\n'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "추가 데이터셋을 담을 디렉토리를 만들어 줍니다.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "# 추가 데이터셋을 담을 디렉토리\n",
    "if not os.path.exists(\"third_party_dataset\"):\n",
    "    os.mkdir(\"third_party_dataset\")\n",
    "\n",
    "if not os.path.exists(\"pre_trained_weight\"):\n",
    "    os.mkdir(\"pre_trained_weight\")\n",
    "\n",
    "\"\"\"\n",
    "이후에는 대회측 배포 데이터셋을 이 deep-text-recognition 디렉토리 내에 받아 open 이라는 디렉토리를 만들어 줍니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n이제 추가 데이터에서 추출하여 최종 데이터셋을 준비해봅시다.\\n밸런싱을 위해 각각의 도메인(가로형, 세로형 등등)의 압축 파일은 1개의 zip 파일만 이용할 것입니다.\\n다만 '책' 도메인은 세부 도메인들의 숫자가 전부 작으므로 모두 이용할 예정입니다.\\n\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset 준비\n",
    "먼저 https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=105\n",
    "의 데이터셋을 다운받습니다. 그리고 third_party_dataset 디렉토리에 받아줍니다.\n",
    "zip 파일을 모두 해제해 줍니다.\n",
    "이제 이 추가 데이터셋과 대회 측 배포 데이터셋을 이용해 학습 데이터셋을 구축하겠습니다.\n",
    "최종 학습 데이터셋의 인풋들은 open/final 안에 둘 예정입니다.\n",
    "\"\"\"\n",
    "\n",
    "# 최종 데이터셋을 담을 디렉토리\n",
    "if not os.path.exists(\"open/final\"):\n",
    "    os.mkdir(\"open/final\")\n",
    "\n",
    "# 최종 데이터셋의 이미지를 담을 디렉토리\n",
    "if not os.path.exists(\"open/final/images\"):\n",
    "    os.mkdir(\"open/final/images\")\n",
    "\n",
    "if not os.path.exists(\"open/final/annotation\"):\n",
    "    os.mkdir(\"open/final/annotation\")\n",
    "\n",
    "if not os.path.exists(\"open/final/images/train\"):\n",
    "    os.mkdir(\"open/final/images/train\")\n",
    "\n",
    "if not os.path.exists(\"open/final/images/val\"):\n",
    "    os.mkdir(\"open/final/images/val\")\n",
    "\n",
    "\"\"\"\n",
    "이제 추가 데이터에서 추출하여 최종 데이터셋을 준비해봅시다.\n",
    "밸런싱을 위해 각각의 도메인(가로형, 세로형 등등)의 압축 파일은 1개의 zip 파일만 이용할 것입니다.\n",
    "다만 '책' 도메인은 세부 도메인들의 숫자가 전부 작으므로 모두 이용할 예정입니다.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import csv\n",
    "from glob import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "def check_available(string):\n",
    "    gt = list('가각간갇갈감갑값갓강갖같갚갛개객걀걔거걱건걷걸검겁것겉게겨격겪견결겹경곁계고곡곤곧골곰곱곳공과관광괜괴굉교구국군굳굴굵굶굽궁권귀귓규균귤그극근글긁금급긋긍기긴길김깅깊까깍깎깐깔깜깝깡깥깨꺼꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꽂꽃꽉꽤꾸꾼꿀꿈뀌끄끈끊끌끓끔끗끝끼낌나낙낚난날낡남납낫낭낮낯낱낳내냄냇냉냐냥너넉넌널넓넘넣네넥넷녀녁년념녕노녹논놀놈농높놓놔뇌뇨누눈눕뉘뉴늄느늑는늘늙능늦늬니닐님다닥닦단닫달닭닮담답닷당닿대댁댐댓더덕던덜덟덤덥덧덩덮데델도독돈돌돕돗동돼되된두둑둘둠둡둥뒤뒷드득든듣들듬듭듯등디딩딪따딱딴딸땀땅때땜떠떡떤떨떻떼또똑뚜뚫뚱뛰뜨뜩뜯뜰뜻띄라락란람랍랑랗래랜램랫략량러럭런럴럼럽럿렁렇레렉렌려력련렬렵령례로록론롬롭롯료루룩룹룻뤄류륙률륭르른름릇릎리릭린림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맺머먹먼멀멈멋멍멎메멘멩며면멸명몇모목몬몰몸몹못몽묘무묵묶문묻물뭄뭇뭐뭘뭣므미민믿밀밉밌및밑바박밖반받발밝밟밤밥방밭배백뱀뱃뱉버번벌범법벗베벤벨벼벽변별볍병볕보복볶본볼봄봇봉뵈뵙부북분불붉붐붓붕붙뷰브븐블비빌빔빗빚빛빠빡빨빵빼뺏뺨뻐뻔뻗뼈뼉뽑뿌뿐쁘쁨사삭산살삶삼삿상새색샌생샤서석섞선설섬섭섯성세섹센셈셋셔션소속손솔솜솟송솥쇄쇠쇼수숙순숟술숨숫숭숲쉬쉰쉽슈스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌍쌓써썩썰썹쎄쏘쏟쑤쓰쓴쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗앙앞애액앨야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇엉엊엌엎에엔엘여역연열엷염엽엿영옆예옛오옥온올옮옳옷옹와완왕왜왠외왼요욕용우욱운울움웃웅워원월웨웬위윗유육율으윽은을음응의이익인일읽잃임입잇있잊잎자작잔잖잘잠잡잣장잦재쟁쟤저적전절젊점접젓정젖제젠젯져조족존졸좀좁종좋좌죄주죽준줄줌줍중쥐즈즉즌즐즘증지직진질짐집짓징짙짚짜짝짧째쨌쩌쩍쩐쩔쩜쪽쫓쭈쭉찌찍찢차착찬찮찰참찻창찾채책챔챙처척천철첩첫청체쳐초촉촌촛총촬최추축춘출춤춥춧충취츠측츰층치칙친칠침칫칭카칸칼캄캐캠커컨컬컴컵컷케켓켜코콘콜콤콩쾌쿄쿠퀴크큰클큼키킬타탁탄탈탑탓탕태택탤터턱턴털텅테텍텔템토톤톨톱통퇴투툴툼퉁튀튜트특튼튿틀틈티틱팀팅파팎판팔팝패팩팬퍼퍽페펜펴편펼평폐포폭폰표푸푹풀품풍퓨프플픔피픽필핏핑하학한할함합항해핵핸햄햇행향허헌험헤헬혀현혈협형혜호혹혼홀홈홉홍화확환활황회획횟횡효후훈훌훔훨휘휴흉흐흑흔흘흙흡흥흩희흰히힘큐빈를숯될앤윤넬힐핌킨샷숍댄펫뚝괘샐봐텐릴닝옵냈롱쌈짇듦엣펙핀릉셀잼잉샨쉐갤땡떙팥딕펠탭샘뢰첨뜸뀐탐딧닉킥밍뽀멜핫뮤펍왁캇겠룸엠았쌤굿쥬튤킴셜톡융짬뽕뎀옻픈빙퀵겐캔랭폴겔갔랩찹캬냠캡셰솝맹했짱샵뭔킹띠퀸웰륨륜쿱돔혁켐뱅숴찜죠틸롤옴빅랙앵밸씽젤댕쫄웹힌헨쉼팜듀뇽썬왔뽈켈쟈덴뎅닙큘햬렛쿡캣쉿었젝앰팰휼윌펌쿨흠맴폼첸곽칡뷔딜팡꿔봤츄랐썸딘왓틴헸슉깃엑헝쎈옙톰낸였쩡셨윈믹멧삽핍텀켄훠궈넙줘샬쏠헥귝벡샹뻘뽁삘젬웠캘칵쉴닌헛쏭셉랄샾랬꼰밴둔툰눔혐엮벧넝딥졌뭉겅쫌즙룡팽쯤쵸슐퀘뮈팍셸풋쭌캉쁠뎃똘쑝훙챌젼떳뻬쿤렐퀼쁜겼낵쎼닛콕잭밧펀윙튬렘탉꽁켑됩벚렙긷탠 ')\n",
    "    return_val = True\n",
    "    for word in string:\n",
    "        if word not in gt:\n",
    "            return_val = False\n",
    "            break\n",
    "    if len(string) > 25:\n",
    "        return_val = False\n",
    "    return return_val\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    return json.load(open(path, 'r'))\n",
    "\n",
    "\n",
    "def run(\n",
    "    path,\n",
    "    info_json_path_root,\n",
    "    index, mode\n",
    "):\n",
    "    image = Image.open(path)\n",
    "    save_root_path = f\"{os.getcwd()}/open/final/images\"\n",
    "\n",
    "    info_json_path = f\"{info_json_path_root}/{path.split('/')[-1].split('.')[0] + '.json'}\"\n",
    "\n",
    "    info_json = read_json(info_json_path)\n",
    "    word_font = info_json[\"metadata\"][0][\"wordfont\"]\n",
    "    info_json = info_json[\"annotations\"]\n",
    "\n",
    "    for info in info_json:\n",
    "        bbox = info[\"bbox\"]\n",
    "        label = info[\"text\"]\n",
    "\n",
    "        if not check_available(label):\n",
    "            continue\n",
    "        croppedImage=image.crop((bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]))\n",
    "        index += 1\n",
    "        croppedImage.save(f\"{save_root_path}/{mode}/{index}.png\")\n",
    "        with open(f'{os.getcwd()}/open/final/annotation/gt_{mode}.txt', 'a', encoding='utf-8', newline='') as f:\n",
    "            tw = csv.writer(f, delimiter='\\t')\n",
    "            tw.writerow([f'{mode}/{index}.png', label])\n",
    "        f.close()\n",
    "    return index\n",
    "\n",
    "\n",
    "def extract_single_type(\n",
    "    mode,index, key,\n",
    "    image_root_root, direc,\n",
    "    info_json_path_root, \n",
    "):\n",
    "    images_list = []\n",
    "    extensions = ['*.jpg', '*.JPG', '*.jpeg', '*.png', '*.PNG']\n",
    "\n",
    "    if \"/\" in direc:\n",
    "        left_path = direc.split(\"/\")[0]\n",
    "        image_root = f\"{image_root_root}/{left_path}\"\n",
    "        if not os.path.exists(f\"{image_root_root}/data_{left_path[4:]}\"):\n",
    "            os.rename(image_root, f\"{image_root_root}/data_{left_path[4:]}\")\n",
    "        image_root = f\"{image_root_root}/data_{left_path[4:]}/{direc.split('/')[1]}\"\n",
    "    else:\n",
    "        image_root = f\"{image_root_root}/{direc}\"\n",
    "        if not os.path.exists(f\"{image_root_root}/data_{direc[4:]}\"):\n",
    "            os.rename(image_root, f\"{image_root_root}/data_{direc[4:]}\")\n",
    "        image_root = f\"{image_root_root}/data_{direc[4:]}\"\n",
    "\n",
    "    for extension in extensions:\n",
    "        images_list.extend(glob(f\"{image_root}/{extension}\"))\n",
    "    print(f\"{mode}의 {image_root} 처리중...\")\n",
    "    \n",
    "    for i, image_path in enumerate(tqdm(images_list, total=len(images_list))):\n",
    "        index = run(\n",
    "            image_path,\n",
    "            info_json_path_root,\n",
    "            index, mode\n",
    "        )\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#학습 데이터셋 추출\n",
    "\"\"\"\n",
    "***************************중요***************************\n",
    "원본 데이터셋의 디렉토리 네이밍 중 몇가지가 잘못된 점이 있습니다(원본 데이터셋 문제)\n",
    "\n",
    "따라서 프로세스의 효율성을 위해 네이밍을 꼭 변경해주셔야 하는 것이 있습니다\n",
    "\n",
    "\"third_party_dataset/야외 실제 촬영 한글 이미지/Training/[라벨]Training/1.간판/5.실내안내판\"\n",
    "내의 \"새 폴더\" 와 \"새 폴더 (2)\"디렉토리명을 변경해주셔야 합니다\n",
    "\n",
    "\"새 폴더\" 를 \"실내안내판1\"\n",
    "\"새 폴더 (2)\"를 \"실내안내판2\"\n",
    "로 **반드시** 변경을 해주셔야 합니다.\n",
    "\n",
    "또한 glob에서 불러올 때 한글디렉토리 명이 문제를 꽤 일으켰습니다\n",
    "\n",
    "변경을 하신 후 이 블록을 실행하시면 됩니다\n",
    "***************************중요***************************\n",
    "\"\"\"\n",
    "\n",
    "mode = \"train\"\n",
    "index = 0\n",
    "image_root = f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Training\"\n",
    "\n",
    "if not os.path.exists(f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Training/label_Training\"):\n",
    "    os.rename(f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Training/[라벨]Training\", f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Training/label_Training\")\n",
    "\n",
    "label_root = f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Training/label_Training\"\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_현수막_원천데이터\", f\"{label_root}/1.간판/8.현수막\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_실내간판_원천데이터1\", f\"{label_root}/1.간판/4.실내간판/실내간판1\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_돌출간판_원천데이터1\", f\"{label_root}/1.간판/2.돌출간판/돌출간판1\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_세로형간판_원천데이터\", f\"{label_root}/1.간판/3.세로형간판\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_가로형간판_원천데이터1\", f\"{label_root}/1.간판/1.가로형간판/가로형간판1\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_실내안내판_원천데이터1\", f\"{label_root}/1.간판/5.실내안내판/실내안내판1\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_지주이용간판_원천데이터\", f\"{label_root}/1.간판/6.지주이용간판\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_간판_창문이용광고물_원천데이터\", f\"{label_root}/1.간판/7.창문이용광고물\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지1/01.총류\", f\"{label_root}/2.책표지/01.총류\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지1/02.철학\", f\"{label_root}/2.책표지/02.철학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지1/03.종교\", f\"{label_root}/2.책표지/03.종교\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지1/04.사회과학\", f\"{label_root}/2.책표지/04.사회과학\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지2/05.자연과학\", f\"{label_root}/2.책표지/05.자연과학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지2/06.기술과학\", f\"{label_root}/2.책표지/06.기술과학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지2/07.예술\", f\"{label_root}/2.책표지/07.예술\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지2/08.언어\", f\"{label_root}/2.책표지/08.언어\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지3/09.문학\", f\"{label_root}/2.책표지/09.문학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지3/10.역사\", f\"{label_root}/2.책표지/10.역사\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Training_책표지3/11.기타\", f\"{label_root}/2.책표지/11.기타\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "***************************중요***************************\n",
    "이 학습 데이터셋에 대회측에서 배포한 데이터셋을 추가해줍시다.\n",
    "그전에 먼저 해주셔야 할 작업이 있습니다.\n",
    "먼저 open/train 의 '모든' 이미지들을 위의 과정으로 만들어진 디렉토리인 \n",
    "'open/final/images/train'으로 복사 해주셔야 합니다.\n",
    "복사를 완료하셨다면 이 셀을 실행하시면 됩니다.\n",
    "***************************중요***************************\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "save_path = f\"{os.getcwd()}/open/final/annotation/gt_train.txt\"\n",
    "\n",
    "reader = csv.reader(open(f\"{os.getcwd()}/open/train.csv\", 'r'))\n",
    "d = {}\n",
    "for index, row in enumerate(reader):\n",
    "   if index > 0:\n",
    "    k, v = row\n",
    "    k = k[2:]\n",
    "    with open(f'{os.getcwd()}/open/final/annotation/gt_train.txt', 'a', encoding='utf-8', newline='') as f:\n",
    "      tw = csv.writer(f, delimiter='\\t')\n",
    "      tw.writerow([k, v])\n",
    "   f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "import csv\n",
    "from glob import glob\n",
    "import time\n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Validation 데이터용 추출 함수\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def check_available(string):\n",
    "    gt = list('가각간갇갈감갑값갓강갖같갚갛개객걀걔거걱건걷걸검겁것겉게겨격겪견결겹경곁계고곡곤곧골곰곱곳공과관광괜괴굉교구국군굳굴굵굶굽궁권귀귓규균귤그극근글긁금급긋긍기긴길김깅깊까깍깎깐깔깜깝깡깥깨꺼꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꽂꽃꽉꽤꾸꾼꿀꿈뀌끄끈끊끌끓끔끗끝끼낌나낙낚난날낡남납낫낭낮낯낱낳내냄냇냉냐냥너넉넌널넓넘넣네넥넷녀녁년념녕노녹논놀놈농높놓놔뇌뇨누눈눕뉘뉴늄느늑는늘늙능늦늬니닐님다닥닦단닫달닭닮담답닷당닿대댁댐댓더덕던덜덟덤덥덧덩덮데델도독돈돌돕돗동돼되된두둑둘둠둡둥뒤뒷드득든듣들듬듭듯등디딩딪따딱딴딸땀땅때땜떠떡떤떨떻떼또똑뚜뚫뚱뛰뜨뜩뜯뜰뜻띄라락란람랍랑랗래랜램랫략량러럭런럴럼럽럿렁렇레렉렌려력련렬렵령례로록론롬롭롯료루룩룹룻뤄류륙률륭르른름릇릎리릭린림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맺머먹먼멀멈멋멍멎메멘멩며면멸명몇모목몬몰몸몹못몽묘무묵묶문묻물뭄뭇뭐뭘뭣므미민믿밀밉밌및밑바박밖반받발밝밟밤밥방밭배백뱀뱃뱉버번벌범법벗베벤벨벼벽변별볍병볕보복볶본볼봄봇봉뵈뵙부북분불붉붐붓붕붙뷰브븐블비빌빔빗빚빛빠빡빨빵빼뺏뺨뻐뻔뻗뼈뼉뽑뿌뿐쁘쁨사삭산살삶삼삿상새색샌생샤서석섞선설섬섭섯성세섹센셈셋셔션소속손솔솜솟송솥쇄쇠쇼수숙순숟술숨숫숭숲쉬쉰쉽슈스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌍쌓써썩썰썹쎄쏘쏟쑤쓰쓴쓸씀씌씨씩씬씹씻아악안앉않알앓암압앗앙앞애액앨야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇엉엊엌엎에엔엘여역연열엷염엽엿영옆예옛오옥온올옮옳옷옹와완왕왜왠외왼요욕용우욱운울움웃웅워원월웨웬위윗유육율으윽은을음응의이익인일읽잃임입잇있잊잎자작잔잖잘잠잡잣장잦재쟁쟤저적전절젊점접젓정젖제젠젯져조족존졸좀좁종좋좌죄주죽준줄줌줍중쥐즈즉즌즐즘증지직진질짐집짓징짙짚짜짝짧째쨌쩌쩍쩐쩔쩜쪽쫓쭈쭉찌찍찢차착찬찮찰참찻창찾채책챔챙처척천철첩첫청체쳐초촉촌촛총촬최추축춘출춤춥춧충취츠측츰층치칙친칠침칫칭카칸칼캄캐캠커컨컬컴컵컷케켓켜코콘콜콤콩쾌쿄쿠퀴크큰클큼키킬타탁탄탈탑탓탕태택탤터턱턴털텅테텍텔템토톤톨톱통퇴투툴툼퉁튀튜트특튼튿틀틈티틱팀팅파팎판팔팝패팩팬퍼퍽페펜펴편펼평폐포폭폰표푸푹풀품풍퓨프플픔피픽필핏핑하학한할함합항해핵핸햄햇행향허헌험헤헬혀현혈협형혜호혹혼홀홈홉홍화확환활황회획횟횡효후훈훌훔훨휘휴흉흐흑흔흘흙흡흥흩희흰히힘큐빈를숯될앤윤넬힐핌킨샷숍댄펫뚝괘샐봐텐릴닝옵냈롱쌈짇듦엣펙핀릉셀잼잉샨쉐갤땡떙팥딕펠탭샘뢰첨뜸뀐탐딧닉킥밍뽀멜핫뮤펍왁캇겠룸엠았쌤굿쥬튤킴셜톡융짬뽕뎀옻픈빙퀵겐캔랭폴겔갔랩찹캬냠캡셰솝맹했짱샵뭔킹띠퀸웰륨륜쿱돔혁켐뱅숴찜죠틸롤옴빅랙앵밸씽젤댕쫄웹힌헨쉼팜듀뇽썬왔뽈켈쟈덴뎅닙큘햬렛쿡캣쉿었젝앰팰휼윌펌쿨흠맴폼첸곽칡뷔딜팡꿔봤츄랐썸딘왓틴헸슉깃엑헝쎈옙톰낸였쩡셨윈믹멧삽핍텀켄훠궈넙줘샬쏠헥귝벡샹뻘뽁삘젬웠캘칵쉴닌헛쏭셉랄샾랬꼰밴둔툰눔혐엮벧넝딥졌뭉겅쫌즙룡팽쯤쵸슐퀘뮈팍셸풋쭌캉쁠뎃똘쑝훙챌젼떳뻬쿤렐퀼쁜겼낵쎼닛콕잭밧펀윙튬렘탉꽁켑됩벚렙긷탠 ')\n",
    "    return_val = True\n",
    "    for word in string:\n",
    "        if word not in gt:\n",
    "            return_val = False\n",
    "            break\n",
    "    if len(string) > 25:\n",
    "        return_val = False\n",
    "    return return_val\n",
    "\n",
    "\n",
    "def read_json(path):\n",
    "    return json.load(open(path, 'r'))\n",
    "\n",
    "\n",
    "def run(\n",
    "    path, key,\n",
    "    info_json_path_root,\n",
    "    index, mode\n",
    "):\n",
    "    image = Image.open(path)\n",
    "    save_root_path = f\"{os.getcwd()}/open/final/images\"\n",
    "\n",
    "    info_json_path = f\"{info_json_path_root}/{path.split('/')[-1].split('.')[0] + '.json'}\"\n",
    "\n",
    "    info_json = read_json(info_json_path)\n",
    "    word_font = info_json[\"metadata\"][0][\"wordfont\"]\n",
    "    info_json = info_json[\"annotations\"]\n",
    "\n",
    "    for info in info_json:\n",
    "        bbox = info[\"bbox\"]\n",
    "        label = info[\"text\"]\n",
    "\n",
    "        if not check_available(label):\n",
    "            continue\n",
    "        croppedImage=image.crop((bbox[0],bbox[1],bbox[0]+bbox[2],bbox[1]+bbox[3]))\n",
    "        index += 1\n",
    "        croppedImage.save(f\"{save_root_path}/{mode}/{index}.png\")\n",
    "        with open(f'{os.getcwd()}/open/final/annotation/gt_{mode}_{key}.txt', 'a', encoding='utf-8', newline='') as f:\n",
    "            tw = csv.writer(f, delimiter='\\t')\n",
    "            tw.writerow([f'{mode}/{index}.png', label])\n",
    "        f.close()\n",
    "        with open(f'{os.getcwd()}/open/final/annotation/gt_{mode}_font_{key}.txt', 'a', encoding='utf-8', newline='') as f:\n",
    "            tw = csv.writer(f, delimiter='\\t')\n",
    "            tw.writerow([f'{mode}/{index}.png', word_font])\n",
    "        f.close()\n",
    "    return index\n",
    "\n",
    "\n",
    "def extract_single_type(\n",
    "    mode,index, key,\n",
    "    image_root_root, direc,\n",
    "    info_json_path_root,\n",
    "):\n",
    "    images_list = []\n",
    "    extensions = ['*.jpg', '*.JPG', '*.jpeg', '*.png', '*.PNG']\n",
    "\n",
    "    if '원' in direc:\n",
    "        if \"/\" in direc:\n",
    "            left_path = direc.split(\"/\")[0]\n",
    "            image_root = f\"{image_root_root}/{left_path}\"\n",
    "            if not os.path.exists(f\"{image_root_root}/data_{left_path[4:]}\"):\n",
    "                os.rename(image_root, f\"{image_root_root}/data_{left_path[4:]}\")\n",
    "            image_root = f\"{image_root_root}/data_{left_path[4:]}/{direc.split('/')[1]}\"\n",
    "        else:\n",
    "            image_root = f\"{image_root_root}/{direc}\"\n",
    "            if not os.path.exists(f\"{image_root_root}/data_{direc[4:]}\"):\n",
    "                os.rename(image_root, f\"{image_root_root}/data_{direc[4:]}\")\n",
    "            image_root = f\"{image_root_root}/data_{direc[4:]}\"\n",
    "    else:\n",
    "        image_root = f\"{image_root_root}/{direc}\"\n",
    "\n",
    "    for extension in extensions:\n",
    "        images_list.extend(glob(f\"{image_root}/{extension}\"))\n",
    "    print(f\"{mode}의 {image_root} 처리중...\")\n",
    "    \n",
    "    for i, image_path in enumerate(tqdm(images_list, total=len(images_list))):\n",
    "        index = run(\n",
    "            image_path, key,\n",
    "            info_json_path_root,\n",
    "            index, mode\n",
    "        )\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#검증 데이터셋 추출\n",
    "\n",
    "mode = \"val\"\n",
    "index = 0\n",
    "image_root = f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Validation\"\n",
    "\n",
    "if not os.path.exists(f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Validation/label_Validation\"):\n",
    "    os.rename(f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Validation/[라벨]Validation\", f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Validation/label_Validation\")\n",
    "\n",
    "label_root = f\"{os.getcwd()}/third_party_dataset/야외 실제 촬영 한글 이미지/Validation/label_Validation\"\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'sero',\n",
    "    f\"{image_root}\", \"[원천]Validation_간판2/03.세로형간판\", f\"{label_root}/1.간판/3.세로형간판\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'garo',\n",
    "    f\"{image_root}\", \"01.가로형간판\", f\"{label_root}/1.간판/1.가로형간판\"\n",
    ")\n",
    "\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/01.총류\", f\"{label_root}/2.책표지/01.총류\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/02.철학\", f\"{label_root}/2.책표지/02.철학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/03.종교\", f\"{label_root}/2.책표지/03.종교\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/04.사회과학\", f\"{label_root}/2.책표지/04.사회과학\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/05.자연과학\", f\"{label_root}/2.책표지/05.자연과학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/06.기술과학\", f\"{label_root}/2.책표지/06.기술과학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/07.예술\", f\"{label_root}/2.책표지/07.예술\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/08.언어\", f\"{label_root}/2.책표지/08.언어\"\n",
    ")\n",
    "\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/09.문학\", f\"{label_root}/2.책표지/09.문학\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/10.역사\", f\"{label_root}/2.책표지/10.역사\"\n",
    ")\n",
    "index = extract_single_type(\n",
    "    mode, index, 'book',\n",
    "    f\"{image_root}\", \"[원천]Validation_책표지/11.기타\", f\"{label_root}/2.책표지/11.기타\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "total_dict = {}\n",
    "\n",
    "def read_gt_val_results(\n",
    "    path, total_dict\n",
    "):\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            (key, value)=line.split(sep='\\t')\n",
    "            if value[-1] == \"\\n\":\n",
    "                value = value[:-1]\n",
    "            total_dict[key] = value\n",
    "    f.close()\n",
    "    return total_dict\n",
    "\n",
    "total_dict = read_gt_val_results(f'{os.getcwd()}/open/final/annotation/gt_val_garo.txt', total_dict)\n",
    "total_dict = read_gt_val_results(f'{os.getcwd()}/open/final/annotation/gt_val_sero.txt', total_dict)\n",
    "total_dict = read_gt_val_results(f'{os.getcwd()}/open/final/annotation/gt_val_book.txt', total_dict)\n",
    "\n",
    "final_dict_to_save_key = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "활자체: 24587\n",
      "캘리그라피: 5011\n"
     ]
    }
   ],
   "source": [
    "#val garo\n",
    "type = \"garo\"\n",
    "font_dict={}\n",
    "with open(f'{os.getcwd()}/open/final/annotation/gt_val_font_garo.txt') as f:\n",
    "    for line in f:\n",
    "        (key, value)=line.split(sep='\\t')\n",
    "        if value[-1] == \"\\n\":\n",
    "            value = value[:-1]\n",
    "        if value != '':\n",
    "            if value not in font_dict:\n",
    "                font_dict[value] = []\n",
    "            font_dict[value].append(key)\n",
    "\n",
    "for key in list(font_dict.keys()):\n",
    "    print(f\"{key}: {len(font_dict[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n"
     ]
    }
   ],
   "source": [
    "a = font_dict[\"활자체\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(2000)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "a = font_dict[\"캘리그라피\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(2000)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "\n",
    "print(len(final_dict_to_save_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "활자체: 4407\n",
      "캘리그라피: 647\n"
     ]
    }
   ],
   "source": [
    "#val sero\n",
    "type = \"sero\"\n",
    "font_dict={}\n",
    "with open(f'{os.getcwd()}/open/final/annotation/gt_val_font_sero.txt') as f:\n",
    "    for line in f:\n",
    "        (key, value)=line.split(sep='\\t')\n",
    "        if value[-1] == \"\\n\":\n",
    "            value = value[:-1]\n",
    "        if value != '':\n",
    "            if value not in font_dict:\n",
    "                font_dict[value] = []\n",
    "            font_dict[value].append(key)\n",
    "\n",
    "for key in list(font_dict.keys()):\n",
    "    print(f\"{key}: {len(font_dict[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "a = font_dict[\"캘리그라피\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(647)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "a = font_dict[\"활자체\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(3000-647)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "\n",
    "print(len(final_dict_to_save_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "활자체: 8656\n",
      "캘리그라피: 1166\n"
     ]
    }
   ],
   "source": [
    "#val book\n",
    "type = \"book\"\n",
    "font_dict={}\n",
    "with open(f'{os.getcwd()}/open/final/annotation/gt_val_font_book.txt') as f:\n",
    "    for line in f:\n",
    "        (key, value)=line.split(sep='\\t')\n",
    "        if value[-1] == \"\\n\":\n",
    "            value = value[:-1]\n",
    "        if value != '':\n",
    "            if value not in font_dict:\n",
    "                font_dict[value] = []\n",
    "            font_dict[value].append(key)\n",
    "\n",
    "for key in list(font_dict.keys()):\n",
    "    print(f\"{key}: {len(font_dict[key])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "a = font_dict[\"캘리그라피\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(1166)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "a = font_dict[\"활자체\"]\n",
    "random.shuffle(a)\n",
    "a = a[:(3000-1166)]\n",
    "final_dict_to_save_key.extend(a)\n",
    "\n",
    "\n",
    "print(len(final_dict_to_save_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "final_dict_to_save = {}\n",
    "\n",
    "for member in final_dict_to_save_key:\n",
    "    with open(f'{os.getcwd()}/open/final/annotation/gt_val_total.txt', 'a', encoding='utf-8', newline='') as f:\n",
    "        tw = csv.writer(f, delimiter='\\t')\n",
    "        tw.writerow([member, total_dict[member]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이제 마침내 추출이 완료되었습니다.\n",
    "하지만 아쉽게도 이 그대로 저희 모델에 넣을 수는 없습니다.\n",
    "바로 특정한 형식으로 압축해서 사용해야 하기 때문인데요.\n",
    "아래는 압축하는 방식입니다.\n",
    "\"\"\"\n",
    "\n",
    "if not os.path.exists(f'{os.getcwd()}/dataset'):\n",
    "    os.mkdir(f'{os.getcwd()}/dataset')\n",
    "\n",
    "!python3 create_lmdb_dataset.py --inputPath open/final/images --gtFile open/final/annotation/gt_val_total.txt --outputPath dataset/train\n",
    "!python3 create_lmdb_dataset.py --inputPath open/final/images --gtFile open/final/annotation/gt_train.txt --outputPath dataset/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "데이터셋 준비가 완료 되었으니 영어에 대해 학습된 pre-trained weight를 다운 받습니다.\n",
    "https://www.dropbox.com/sh/j3xmli4di1zuv3s/AADbTu4LF-nMUBmC43_RQ8OGa/TPS-ResNet-BiLSTM-Attn.pth?dl=0\n",
    "이 파일을 다운로드 받아 pre_trained_weight 디렉토리 밑에 넣어 주시면 됩니다.\n",
    "\n",
    "이제 학습을 해보도록 하겠습니다. \n",
    "학습은 saved_models/for_sub/log_train.txt 안에 로깅됩니다.\n",
    "스텝을 약 196500정도 밟았을 때 학습을 멈춰주었습니다.(약 3일정도 train)\n",
    "저희가 학습을 할 때는 학습을 할때는 주피터 노트북으로 하지 않고 nohup으로 백그라운드로 실행한 후,\n",
    "로그를 확인해가며 196500정도 되었을 때 process를 kill 하였기 때문에,\n",
    "서버를 통해 주피터 노트북을 실행하실 경우(이 블록을 실행할 때), 커널이 끊길 경우 학습이 중단될 위험이 있습니다.\n",
    "\n",
    "아래 코드로 주피터 노트북으로 돌릴수 있으나, 저희 팀은 애초에 노트북 커널로 실행하지 않고 백그라운드 프로세스로 실행했을 뿐더러,\n",
    "주피터 노트북 커널은 서버로 서버를 연결해 돌리든, 그냥 하든 끊길 위험이 꽤 크므로 \n",
    "가능하다면 nohup으로 백그라운드 프로세스를 통해 돌려주시길 부탁드립니다.\n",
    "\n",
    "또한 batch size는 변경하지 말아주셨으면 합니다.\n",
    "Batch size가 달라지면 learning rate도 달라져야 하는데, 저희는 저희의 batch size에 대해서만 최적의\n",
    "learning rate을 알기 때문입니다.\n",
    "또한 Batch size가 커질수록 일반화 성능이 떨어진다는 연구도 상당수 있으므로 batch size는 그대로 유지해주셨으면 합니다.\n",
    "\n",
    "주피터 커널로 학습을 돌리는 방법, nohup으로 돌리는 방법 둘다 설명 드리겠습니다\n",
    "\n",
    "1. 주피터 커널로 학습을 돌리는 방법\n",
    "    Step1. 이 블록을 실행한다.\n",
    "    Step2. 약 3일간 기다리며 수시로 로그 파일(saved_models/for_sub/log_train.txt)을 확인한다.\n",
    "    Step3. 스텝이 196500정도 되었을 때 이 블록을 중단한다.\n",
    "2. nohup으로 학습을 돌리는 방법\n",
    "    Step1. 터미널에서 이 디렉토리(deep-text-recognition)으로 이동한다.\n",
    "    Step2. 터미널에 nohup sh train.sh > train.out & 으로 백그라운드 프로세스를 실행합니다.\n",
    "    Step3. 약 3일간 기다리며 수시로 로그 파일(saved_models/for_sub/log_train.txt)을 확인합니다.\n",
    "    Step4. 스텝이 196500정도 되었을 때 ps -ef | grep train으로 프로세스를 찾아 kill 해줍니다.\n",
    "\n",
    "(총 학습을 돌릴 step 수를 196500으로 설정할 수 없는 이유가 learning rate 스케줄러가 저희가 학습할때와 바뀔수도 있기 때문입니다.) \n",
    "\"\"\"\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 train.py --train_data dataset/train --valid_data dataset/validation --Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn --data_filtering_off --saved_model \"pre_trained_weight/TPS-ResNet-BiLSTM-Attn.pth\" --workers 0 --imgH 200 --imgW 200 --exp_name \"./for_sub\" --batch_size 64 --FT --valInterval 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이제 학습 완료된 weight로 inference를 진행해봅시다.\n",
    "대회 데이터셋이 디렉토리 open 안에 test(inference) 데이터가 있습니다.\n",
    "inference 코드는 아래와 같습니다. 먼저 방금 학습을 완료한 weight로 추론을 해보겠습니다.\n",
    "\"\"\"\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 demo.py \\\n",
    "--Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn \\\n",
    "--image_folder open/test \\\n",
    "--saved_model saved_models/for_sub/best_accuracy.pth \\\n",
    "--workers 0 --imgH 200 --imgW 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n방금학습된 모델 weight로 inference한 결과는 현재 디렉토리의 result_from_trained.csv에 저장되어 있습니다.\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json(path):\n",
    "    return json.load(open(path, 'r'))\n",
    "\n",
    "dict = read_json(\"result.json\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"open/sample_submission.csv\")\n",
    "list = []\n",
    "count = 0\n",
    "for index in range(len(df)):\n",
    "  data = df.iloc[index]\n",
    "  file_name = data[\"img_path\"]\n",
    "  if f\"open/{file_name[2:]}\" not in dict:\n",
    "    count += 0\n",
    "  else:\n",
    "    list.append(dict[f\"open/{file_name[2:]}\"])\n",
    "    count += 1\n",
    "df[\"text\"] = list\n",
    "df.to_csv(\"result_from_trained.csv\", index=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "방금학습된 모델 weight로 inference한 결과는 현재 디렉토리의 result_from_trained.csv에 저장되어 있습니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "이번엔 이전에 저희가 제출했던 모델 weight를 이용해 inference 해보겠습니다.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0 python3 demo.py \\\n",
    "--Transformation TPS --FeatureExtraction ResNet --SequenceModeling BiLSTM --Prediction Attn \\\n",
    "--image_folder open/test \\\n",
    "--saved_model submitted_best_accuracy.pth \\\n",
    "--workers 0 --imgH 200 --imgW 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_json(path):\n",
    "    return json.load(open(path, 'r'))\n",
    "\n",
    "dict = read_json(\"result.json\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"open/sample_submission.csv\")\n",
    "list = []\n",
    "count = 0\n",
    "for index in range(len(df)):\n",
    "  data = df.iloc[index]\n",
    "  file_name = data[\"img_path\"]\n",
    "  if f\"open/{file_name[2:]}\" not in dict:\n",
    "    count += 0\n",
    "  else:\n",
    "    list.append(dict[f\"open/{file_name[2:]}\"])\n",
    "    count += 1\n",
    "df[\"text\"] = list\n",
    "df.to_csv(\"result_from_submit_weight.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec3e5be10dc1a03c05a90fbbabd71f073af29753090ff929cb1d2cab81dbf16a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
